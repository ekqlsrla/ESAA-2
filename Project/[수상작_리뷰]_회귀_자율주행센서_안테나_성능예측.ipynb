{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S8Pf_1OT7f6vYgR4FLCKAfn384PdCf_Y",
      "authorship_tag": "ABX9TyPc6blkZeANKaY5IS8r5E0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekqlsrla/ESAA-2/blob/main/Project/%5B%EC%88%98%EC%83%81%EC%9E%91_%EB%A6%AC%EB%B7%B0%5D_%ED%9A%8C%EA%B7%80_%EC%9E%90%EC%9C%A8%EC%A3%BC%ED%96%89%EC%84%BC%EC%84%9C_%EC%95%88%ED%85%8C%EB%82%98_%EC%84%B1%EB%8A%A5%EC%98%88%EC%B8%A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **자율주행 센서의 안테나 성능 예측 AI 경진대회**\n",
        "\n",
        "* 알고리즘|정형|회귀|자율주행|NRMSE"
      ],
      "metadata": {
        "id": "ng4sMdgDaiqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 코드 실행환경"
      ],
      "metadata": {
        "id": "4uAT8hJNaq9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) 필요한 라이브러리 설치\n",
        "\n",
        "* `optuna`\n",
        "  * 하이퍼파라미터 최적화 태스크를 도와줌"
      ],
      "metadata": {
        "id": "FS5Wqv5t0dkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZfb0IZF34wg",
        "outputId": "ddccfe16-1941-46e8-b9e2-5bb6485dc914"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 KB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.21.6)\n",
            "Collecting alembic>=1.5.0\n",
            "  Downloading alembic-1.9.2-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 KB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.4.46)\n",
            "Collecting cmaes>=0.9.1\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (23.0)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (5.10.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from alembic>=1.5.0->optuna) (6.0.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic>=1.5.0->optuna) (3.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.9.2 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optuna 설치\n",
        "!pip install --quiet --no-cache-dir git+https://github.com/optuna/optuna\n",
        "\n",
        "# Catboost 설치\n",
        "!pip install --quiet catboost\n",
        "\n",
        "# XGB GPU 버전 설치\n",
        "!pip uninstall --quiet -y xgboost\n",
        "!pip install --quiet xgboost\n",
        "\n",
        "# LGBM GPU 버전 설치\n",
        "! git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW4TOXm-0E8t",
        "outputId": "ac36e789-4a43-4875-e678-82ca6122de67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for UNKNOWN (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'LightGBM'...\n",
            "remote: Enumerating objects: 28834, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 28834 (delta 33), reused 60 (delta 28), pack-reused 28742\u001b[K\n",
            "Receiving objects: 100% (28834/28834), 20.18 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (21360/21360), done.\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n",
            "Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n",
            "Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n",
            "Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/compute'...\n",
            "remote: Enumerating objects: 21733, done.        \n",
            "remote: Counting objects: 100% (5/5), done.        \n",
            "remote: Compressing objects: 100% (4/4), done.        \n",
            "remote: Total 21733 (delta 1), reused 3 (delta 1), pack-reused 21728        \n",
            "Receiving objects: 100% (21733/21733), 8.51 MiB | 10.62 MiB/s, done.\n",
            "Resolving deltas: 100% (17567/17567), done.\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/eigen'...\n",
            "remote: Enumerating objects: 118085, done.        \n",
            "remote: Counting objects: 100% (1172/1172), done.        \n",
            "remote: Compressing objects: 100% (366/366), done.        \n",
            "remote: Total 118085 (delta 806), reused 1168 (delta 803), pack-reused 116913        \n",
            "Receiving objects: 100% (118085/118085), 103.46 MiB | 24.33 MiB/s, done.\n",
            "Resolving deltas: 100% (97387/97387), done.\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/fast_double_parser'...\n",
            "remote: Enumerating objects: 781, done.        \n",
            "remote: Counting objects: 100% (180/180), done.        \n",
            "remote: Compressing objects: 100% (66/66), done.        \n",
            "remote: Total 781 (delta 124), reused 131 (delta 103), pack-reused 601        \n",
            "Receiving objects: 100% (781/781), 833.52 KiB | 1.74 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/fmt'...\n",
            "remote: Enumerating objects: 31478, done.        \n",
            "remote: Total 31478 (delta 0), reused 0 (delta 0), pack-reused 31478        \n",
            "Receiving objects: 100% (31478/31478), 13.60 MiB | 14.94 MiB/s, done.\n",
            "Resolving deltas: 100% (21326/21326), done.\n",
            "Submodule path 'external_libs/compute': checked out '36350b7de849300bd3d72a05d8bf890ca405a014'\n",
            "Submodule path 'external_libs/eigen': checked out '3147391d946bb4b6c68edd901f2add6ac1f31f8c'\n",
            "Submodule path 'external_libs/fast_double_parser': checked out 'ace60646c02dc54c57f19d644e49a61e7e7758ec'\n",
            "Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n",
            "Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n",
            "remote: Enumerating objects: 20414, done.        \n",
            "remote: Counting objects: 100% (2673/2673), done.        \n",
            "remote: Compressing objects: 100% (753/753), done.        \n",
            "remote: Total 20414 (delta 1973), reused 1959 (delta 1920), pack-reused 17741        \n",
            "Receiving objects: 100% (20414/20414), 12.42 MiB | 15.52 MiB/s, done.\n",
            "Resolving deltas: 100% (15993/15993), done.\n",
            "Cloning into '/content/LightGBM/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n",
            "remote: Enumerating objects: 1352, done.        \n",
            "remote: Counting objects: 100% (196/196), done.        \n",
            "remote: Compressing objects: 100% (105/105), done.        \n",
            "remote: Total 1352 (delta 109), reused 157 (delta 84), pack-reused 1156        \n",
            "Receiving objects: 100% (1352/1352), 7.15 MiB | 15.34 MiB/s, done.\n",
            "Resolving deltas: 100% (881/881), done.\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n",
            "Submodule path 'external_libs/fmt': checked out 'b6f4ceaed0a0a24ccf575fab6c56dd50ccf6f1a9'\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Looking for CL_VERSION_2_2\n",
            "-- Looking for CL_VERSION_2_2 - found\n",
            "-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n",
            "-- OpenCL include directory: /usr/include\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.56.0\") found components: filesystem system \n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LightGBM/LightGBM/build\n",
            "[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_capi_objs.dir/src/c_api.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/cuda/cuda_score_updater.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/boosting/sample_strategy.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/cuda/cuda_utils.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 22%] Built target lightgbm_capi_objs\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_column_data.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_metadata.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_row_data.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/cuda/cuda_tree.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_binary_metric.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_pointwise_metric.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/cuda/cuda_regression_metric.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_binary_objective.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_multiclass_objective.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_rank_objective.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/cuda/cuda_regression_objective.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_best_split_finder.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_data_partition.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_histogram_constructor.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_leaf_splits.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/cuda/cuda_single_gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/data_parallel_tree_learner.cpp:9\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n",
            "   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/feature_parallel_tree_learner.cpp:8\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n",
            "   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.cpp:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n",
            "   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/tree_learner.cpp:7\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n",
            "   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm_objs.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "In file included from \u001b[01m\u001b[K/usr/include/CL/cl.h:32\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/cl.hpp:19\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/config.hpp:16\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/buffer.hpp:14\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/external_libs/compute/include/boost/compute/core.hpp:18\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/gpu_tree_learner.h:33\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/parallel_tree_learner.h:15\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/content/LightGBM/LightGBM/src/treelearner/voting_parallel_tree_learner.cpp:11\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/include/CL/cl_version.h:34:104:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K#pragma message: cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\n",
            "   34 | #pragma message(\"cl_version.h: CL_TARGET_OPENCL_VERSION is not defined. Defaulting to 220 (OpenCL 2.2)\"\u001b[01;36m\u001b[K)\u001b[m\u001b[K\n",
            "      |                                                                                                        \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "[ 92%] Built target lightgbm_objs\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX shared library ../lib_lightgbm.so\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[ 98%] Built target _lightgbm\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../lightgbm\u001b[0m\n",
            "[100%] Built target lightgbm\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "Generating grammar tables from /usr/lib/python3.8/lib2to3/Grammar.txt\n",
            "Generating grammar tables from /usr/lib/python3.8/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/dask.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching 'compile/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n",
            "warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n",
            "warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/dask.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "Installing lib_lightgbm from: ['/content/LightGBM/LightGBM/lib_lightgbm.so']\n",
            "copying /content/LightGBM/LightGBM/lib_lightgbm.so -> /usr/local/lib/python3.8/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/__init__.py to __init__.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/compat.py to compat.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/dask.py to dask.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/libpath.py to libpath.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/callback.py to callback.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/plotting.py to plotting.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/basic.py to basic.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/sklearn.py to sklearn.cpython-38.pyc\n",
            "byte-compiling /usr/local/lib/python3.8/dist-packages/lightgbm/engine.py to engine.cpython-38.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.8/dist-packages/lightgbm-3.3.5.99-py3.8.egg-info\n",
            "running install_scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) 라이브러리 로드"
      ],
      "metadata": {
        "id": "sGyvCmyo1OZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoLars, OrthogonalMatchingPursuit, BayesianRidge, ARDRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import optuna \n",
        "from optuna import Trial, visualization\n",
        "\n",
        "import joblib\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "elDN3oSy1QoQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) 시드 고정"
      ],
      "metadata": {
        "id": "j9vaDqDM1UX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "seed_everything(42) \n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "-DdWbTYn1WRS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터 전처리\n"
      ],
      "metadata": {
        "id": "MPsZR9dk4uoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로드\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ESAA-2/DATA/자율주행_회귀/train.csv')\n",
        "\n",
        "#데이터 분리\n",
        "\n",
        "X_train = train.filter(regex = 'X')\n",
        "Y_train = train.filter(regex = 'Y')"
      ],
      "metadata": {
        "id": "s4fNxwii46xQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결과에 영향 낮은 인자 제거\n",
        "\n",
        "X_train = X_train.drop(['X_04','X_23','X_47','X_48','X_10','X_11','X_02'],axis = 1)\n",
        "\n",
        "#X_33 이상치 제거\n",
        "\n",
        "drop_idx = X_train.loc[X_train['X_33'] > 6].index\n",
        "\n",
        "X_train = X_train.drop(drop_idx, axis = 0)\n",
        "Y_train = Y_train.drop(drop_idx, axis = 0)\n",
        "\n",
        "X_train = X_train.reset_index(drop = True)\n",
        "Y_train = Y_train.reset_index(drop= True)"
      ],
      "metadata": {
        "id": "m-W7OKkK50WW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA 클래스 설정\n",
        "\n",
        "class PCA_transform :\n",
        "  def __init__(self) :\n",
        "    self.cols_list = []\n",
        "    self.pca_list = []\n",
        "    self.n_pca_list = []\n",
        "    self.size = 0\n",
        "\n",
        "  #PCA 클래스의 학습 및 input 값 변환\n",
        "  def fit_transform(self, X_input, col, n_pca) :\n",
        "    X_pca = X_input[col]\n",
        "\n",
        "    pca = PCA(n_components = n_pca)\n",
        "\n",
        "    pca.fit(X_pca)\n",
        "\n",
        "    #데이터프레임으로 자료형 변경\n",
        "    X_pca = pca.transform(X_pca)\n",
        "    X_pca = pd.DataFrame(X_pca, columns = self.naming(n_pca))\n",
        "\n",
        "    X_input = pd.concat([X_input, X_pca], axis = 1)\n",
        "    X_input = X_input.drop(col,axis = 1)\n",
        "\n",
        "    self.cols_list.append(col)\n",
        "    self.pca_list.append(pca)\n",
        "    self.n_pca_list.append(n_pca)\n",
        "    self.size += 1\n",
        "\n",
        "    return X_input\n",
        "\n",
        "  def transform(self,X_input) :\n",
        "    for idx in range(self.size) :\n",
        "      X_input = self._idx_transform(X_input,idx)\n",
        "\n",
        "    return X_input\n",
        "\n",
        "#n번쨰 PCA 변환\n",
        "  def _idx_transform(self,X_input,idx):\n",
        "    X_pca = X_input[self.cols_list[idx]]\n",
        "\n",
        "    #df로 자료형 변경\n",
        "    X_pca = self.pca_list[idx].transofm(X_pca)\n",
        "    X_pca = pd.DataFrame(X_pca, columns = self.naming(self.n_pca_list[idx],idx))\n",
        "\n",
        "    X_input = pd.concat([X_input,X_pca],axis = 1)\n",
        "    X_input = X_input.drop(self.cols_list[idx],axis = 1)\n",
        "\n",
        "    return X_input\n",
        "\n",
        "  #PCA된 칼럼 이름 규칙\n",
        "  def naming(self,number,name = None) :\n",
        "    if (name is None) :\n",
        "      name = self.size\n",
        "    names = []\n",
        "    for idx in range(number) :\n",
        "      names.append(f'PCA_{str(name)}_{idx}')\n",
        "    return names"
      ],
      "metadata": {
        "id": "WmM-pGgY6ViQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optuna로 최적화된 PCA 파라미터 적용\n",
        "\n",
        "pca_5 = PCA_transform()\n",
        "X_train = pca_5.fit_transform(X_train, ['X_13', 'X_14', 'X_15', 'X_16', 'X_17', 'X_18'], 5)\n",
        "X_train = pca_5.fit_transform(X_train, ['X_19', 'X_20', 'X_21', 'X_22'], 2)\n",
        "X_train = pca_5.fit_transform(X_train, ['X_34', 'X_35', 'X_36', 'X_37'], 1)\n",
        "X_train = pca_5.fit_transform(X_train, ['X_41', 'X_42', 'X_43', 'X_44', 'X_45'], 1)\n",
        "X_train = pca_5.fit_transform(X_train, ['X_50', 'X_51', 'X_52', 'X_53', 'X_54', 'X_55', 'X_56'], 2)"
      ],
      "metadata": {
        "id": "J7_DI0D-FItn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 평가산식 정의"
      ],
      "metadata": {
        "id": "BNniBASzFhb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nrmse(y_val, y_pred) :\n",
        "  rmse = mean_squared_error(y_val, y_pred, squared = False)\n",
        "  nrmse = rmse/np.mean(np.abs(y_val))\n",
        "  return nrmse\n",
        "\n",
        "def lg_nrmse(y_val, y_pred) :\n",
        "  y_val = pd.DataFrame(y_val)\n",
        "  y_pred = pd.DataFrame(y_pred)\n",
        "\n",
        "  all_nrmse = []\n",
        "  for idx in range(0,14) :\n",
        "    all_nrmse.append(nrmse(y_val.iloc[:,idx], y_pred.iloc[:,idx]))\n",
        "\n",
        "  score = 1.2*np.sum(all_nrmse[:7]) + 1.0*np.sum(all_nrmse[7:14])\n",
        "  return score"
      ],
      "metadata": {
        "id": "K0xK6DhKFju8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 모델링"
      ],
      "metadata": {
        "id": "1L5UFwKWGo3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) **Cross- Validation**을 통해 학습하는 ML class"
      ],
      "metadata": {
        "id": "ISW9OhzRGqKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CV_ml_model :\n",
        "  #X_train.Y_train을 인자로 받음\n",
        "  def __init__(self, model, X_train, Y_train):\n",
        "    self.model = model\n",
        "    self.name = model().__class__.__name__\n",
        "    self.train_preds = [None] * 14\n",
        "    self.trained_models = [None] * 14\n",
        "    self.test_preds = [None] * 14\n",
        "    self.X_train = self.to_np(X_train)\n",
        "    self.Y_train = self.to_np(Y_train)\n",
        "\n",
        "\n",
        "  \n",
        "  #넘파이 변환\n",
        "  def to_np(self, input):\n",
        "    if (type(input) == pd.core.frame.DataFrame):\n",
        "      return input.to_numpy()\n",
        "    return input\n",
        "\n",
        "  #이름 재설정\n",
        "  def set_name(self, name):\n",
        "    self.name = name\n",
        "\n",
        "  #Y하나에 대해 머신러닝 수행\n",
        "  def y_fit(self, n_folds, y_idx, X_train = None, param = {}, save = False):\n",
        "    if (X_train is None):\n",
        "      X_train = self.X_train #X_train 따로 설정하지 않으면 최초 입력 데이터 적용\n",
        "\n",
        "    #이미 학습된  y_idx -> 일시 학습 중지\n",
        "    if (self.train_preds[y_idx] is not None):\n",
        "      return\n",
        "    \n",
        "    #n_folds 숫자 만큼 학습된 모델이 들어갈 리스트 생성\n",
        "    trained_y_models = [None] * n_folds\n",
        "\n",
        "    #예측 결과 넣을 ndarray\n",
        "    train_fold_pred = np.zeros((X_train.shape[0], 1))\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
        "      X_tr = X_train[train_index]\n",
        "      y_tr = self.Y_train[:,y_idx][train_index]\n",
        "      X_val = X_train[valid_index]\n",
        "\n",
        "      #폴드 세트 내부에서 만들어진 학습 데이터로 기반 모델의 학습 수행\n",
        "      trained_model = self.model(**param).fit(X_tr , y_tr)  \n",
        "      #검증 데이터로 모델 예측 후 데이터 저장\n",
        "      train_fold_pred[valid_index, :] = trained_model.predict(X_val).reshape(-1,1)\n",
        "      #학습 완료된 모델 리스트 내에 저장\n",
        "      trained_y_models[folder_counter] = trained_model\n",
        "\n",
        "    #예측 결과 class 내 저장\n",
        "    self.train_preds[y_idx] = train_fold_pred\n",
        "    self.trained_models[y_idx] = trained_y_models\n",
        "\n",
        "    #학습 완료시 모델을 파일로 저장\n",
        "    if (save):\n",
        "      self.save()\n",
        "\n",
        "#모델 경량화\n",
        "\n",
        "  def slim_y_fit(self, n_folds, y_idx, X_train = None, param = {}, save = False):\n",
        "    if (X_train is None):\n",
        "      X_train = self.X_train\n",
        "\n",
        "    if (self.train_preds[y_idx] is not None):\n",
        "      return\n",
        "\n",
        "  \n",
        "  #ndarray 생성\n",
        "    train_fold_pred = np.zeros((X_train.shape[0], 1))\n",
        "\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train)):\n",
        "      X_tr = X_train[train_index] \n",
        "      y_tr = self.Y_train[:,y_idx][train_index] \n",
        "      X_val = X_train[valid_index]  \n",
        "\n",
        "\n",
        "      trained_model = self.model(**param).fit(X_tr , y_tr)  \n",
        "      train_fold_pred[valid_index, :] = trained_model.predict(X_val).reshape(-1,1)\n",
        "\n",
        "    self.train_preds[y_idx] = train_fold_pred\n",
        "    self.trained_models[y_idx] = [self.model(**param).fit(X_train , self.Y_train[:,y_idx])]\n",
        "\n",
        "    if (save):\n",
        "      self.save()\n",
        "\n",
        "#Y 하나에 대해 예측 수행\n",
        "\n",
        "  def y_predict(self, X_test, y_idx):\n",
        "  #학습 완료된 모델 갯수 확인\n",
        "    size = len(self.trained_models[y_idx])\n",
        "  #ndarray 생성\n",
        "    test_pred = np.zeros((X_test.shape[0], size))\n",
        "\n",
        "  #학습 완료된 모델 갯수만큼 예측 수행\n",
        "    for counter in range(size):\n",
        "      test_pred[:, counter] = self.trained_models[y_idx][counter].predict(X_test)\n",
        "\n",
        "  #예측 결과값에 대해 평균 구함\n",
        "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
        "\n",
        "    self.test_preds[y_idx] = test_pred_mean\n",
        "\n",
        "    return test_pred_mean\n",
        "\n",
        "#머신러닝 훈련 실행\n",
        "\n",
        "  def fit(self, n_folds, use_params = False, params = None, save = False):\n",
        "    print(self.name, 'Started')\n",
        "\n",
        "\n",
        "  #use_params이 참이고, params이 없을 시 class 이름으로 하이퍼파라미터 튜닝된 파라미터 불러오기\n",
        "    if (use_params):\n",
        "      if (params is None):\n",
        "        params = self.load_params(self.name)\n",
        "        print('Params Loaded!')\n",
        "        for y_idx in range(14):\n",
        "          print(y_idx, end= ' ')\n",
        "          self.y_fit(n_folds, y_idx, None, params[y_idx], save)\n",
        "\n",
        "      else:\n",
        "        for y_idx in range(14):\n",
        "          print(y_idx, end= ' ')\n",
        "          self.y_fit(n_folds, y_idx, None, params, save)\n",
        "\n",
        "    else:\n",
        "      for y_idx in range(14):\n",
        "        print(y_idx, end= ' ')\n",
        "        self.y_fit(n_folds, y_idx, save = save)\n",
        "    \n",
        "    print(self.name, 'Trained!')\n",
        "\n",
        "#모델 저장시 하나의 모델로 저장하여 모델 경량화\n",
        "\n",
        "  def slim_fit(self, n_folds, use_params = False, params = None, save = False):\n",
        "    print(self.name, 'Started')\n",
        "\n",
        "\n",
        "    if (use_params):\n",
        "      if (params is None):\n",
        "        params = self.load_params(self.name)\n",
        "        print('Params Loaded!')\n",
        "        for y_idx in range(14):\n",
        "          print(y_idx, end= ' ')\n",
        "          self.slim_y_fit(n_folds, y_idx, None, params[y_idx], save)\n",
        "      else:\n",
        "        for y_idx in range(14):\n",
        "          print(y_idx, end= ' ')\n",
        "          self.slim_y_fit(n_folds, y_idx, None, params, save)\n",
        "\n",
        "    else:\n",
        "      for y_idx in range(14):\n",
        "        print(y_idx, end= ' ')\n",
        "        self.slim_y_fit(n_folds, y_idx, save = save)\n",
        "    \n",
        "    print(self.name, 'Slim!')\n",
        "\n",
        "\n",
        "#학습된 모델을 통해 예측\n",
        "\n",
        "  def predict(self, X_test):\n",
        "    for y_idx in range(14):\n",
        "      self.y_predict(X_test, y_idx)\n",
        "\n",
        "    return np.hstack(self.test_preds)\n",
        "\n",
        "\n",
        "#예측된 Y_train 결과 반환\n",
        "\n",
        "  def get_train_preds(self):\n",
        "    return np.hstack(self.train_preds)\n",
        "\n",
        "#예측된 Y_test 결과 반환\n",
        "\n",
        "  def get_test_preds(self):\n",
        "    return np.hstack(self.test_preds)\n",
        "\n",
        "#nrmse 값 반환\n",
        "  def nrmse(self, y_idx):\n",
        "    rmse = mean_squared_error(self.Y_train[:,y_idx], self.train_preds[y_idx], squared=False)\n",
        "    nrmse = rmse/np.mean(np.abs(self.Y_train[:,y_idx]))\n",
        "    return nrmse\n",
        "\n",
        "#모델의 Cross-validation 점수 반환\n",
        "  def score(self):\n",
        "    all_nrmse = [None] * 14\n",
        "    for y_idx in range(14):\n",
        "      all_nrmse[y_idx] = self.nrmse(y_idx)\n",
        "\n",
        "    score = 1.2 * np.sum(all_nrmse[:7]) + 1.0 * np.sum(all_nrmse[7:14])\n",
        "    return score\n",
        "\n",
        "\n",
        "#모델을 현재 이름으로 저장\n",
        "  def save(self) :\n",
        "    saved_data = self.model, self.name, self.train_preds,self.trained_models, self.test_preds\n",
        "    try : \n",
        "      os.mkdir('/content/drive/MyDrive/Colab Notebooks/ESAA-2/Project' + 'ML_model')\n",
        "    except :\n",
        "      pass\n",
        "  #머신러닝 모델 저장하는 모듈\n",
        "    joblib.dump(saved_data,'/content/drive/MyDrive/Colab Notebooks/ESAA-2/Project','ML_model/saved_' + self.name + '.pkl')\n",
        "  \n",
        "\n",
        "\n",
        "#모델을 현재 이름 혹은 입력한 이름으로 불러오기\n",
        "\n",
        "  def load(self,name = None) :\n",
        "    if (not name) :\n",
        "      name = self.name\n",
        "    try :\n",
        "      loaded_data = joblib.load('/content/drive/MyDrive/Colab Notebooks/ESAA-2/Project' + 'ML_model/saved_' + name + '.pkl')\n",
        "      self.model,self.name, self.train_preds, self.trained_models, self.test_preds = loaded_data\n",
        "      print(self.name, 'Loaded!')\n",
        "\n",
        "      if (self.train_models[13] is None) :\n",
        "        return False #학습 전체가 완료된 모델일시 참 <-> 학습이 남은 모델일시 거짓\n",
        "      else :\n",
        "        return True\n",
        "    except :\n",
        "      print(self.name, 'didnt loaded')\n",
        "      return False\n",
        "\n",
        "\n",
        "#파라미터 불러오기\n",
        "  def load_params(self, name) :\n",
        "    params = [None] * 14\n",
        "    for y_idx in range(14):\n",
        "      try :\n",
        "        load_study = joblib.load('/content/drive/MyDrive/Colab Notebooks/ESAA-2/Project' + 'tune_params/' + name + '/tune_' + str(y_idx) + '.pkl')\n",
        "        params[y_idx] = load_study.best_trial.params\n",
        "      except :\n",
        "        print(name, y_idx,'params didnt loaded')\n",
        "    return params"
      ],
      "metadata": {
        "id": "PG9_hKliGvxI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `np.hstack` : 가로 결합"
      ],
      "metadata": {
        "id": "ZR0XfKa2CB5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) **CV_ml_model**을 확장한 **Stacking ensemble**을 위한 Meta Learning class"
      ],
      "metadata": {
        "id": "9opj-p_eEj0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class meta_ml_model(CV_ml_model):\n",
        "  def __init__(self, model, X_train, Y_train):\n",
        "    super().__init__(model, X_train, Y_train)\n",
        "    self.ml_models = []\n",
        "    self.trained = []\n",
        "\n",
        "  \n",
        "  #훈련된 CV_ml_model을 받음\n",
        "  def add_trained_ml_models(self, *models:CV_ml_model):\n",
        "    for model in models:\n",
        "      self.ml_models.append(model)\n",
        "      self.trained.append(True)\n",
        "    \n",
        "  #머신러닝 모델 추가 훈련 안된 모델함수를 받음\n",
        "\n",
        "  def add_new_ml_models(self, *models):\n",
        "    for model in models:\n",
        "      new_model = CV_ml_model(model, self.X_train, self.Y_train)\n",
        "      self.ml_models.append(new_model)\n",
        "      self.trained.append(False)\n",
        "\n",
        "\n",
        "  #머신러닝 모델 전체 훈련 : 훈련되어있지 않은 모델만 훈련\n",
        "  def fit_ml_model(self, n_folds, save = False):\n",
        "    for idx, model in enumerate(self.ml_models):\n",
        "      if (not self.trained[idx]):\n",
        "        model.fit(n_folds, save=save)\n",
        "        self.trained[idx] = True\n",
        "\n",
        "\n",
        " #class 내에 추가된 머신러닝 모델 전체 불러오기\n",
        "  def load_all_models(self):\n",
        "    for idx, model in enumerate(self.ml_models):\n",
        "      self.trained[idx] = model.load()\n",
        "\n",
        "\n",
        "#class 내에 추가된 머신러닝 모델 전체 저장\n",
        "  def save_all_models(self):\n",
        "    for model in self.ml_models:\n",
        "      model.save()\n",
        "\n",
        "\n",
        "  #훈련 여부 수동 체크\n",
        "  def trained_check(self,idx) :\n",
        "    self.trained[idx] = True\n",
        "\n",
        "  #머신러닝 모델 하나 훈련(파라미터 입력 가능)\n",
        "  def fit_one_ml_model(self, n_folds, model_idx, use_params = False, params = None, save=False):\n",
        "    self.ml_models[model_idx].fit(n_folds, use_params, params, save)\n",
        "    self.trained[model_idx] = True\n",
        "\n",
        "\n",
        "  #머신러닝 모델 하나 경량 훈련(파라미터 입력 가능)\n",
        "  def slim_fit_one_ml_model(self, n_folds, model_idx, use_params = False, params = None, save=False):\n",
        "    self.ml_models[model_idx].slim_fit(n_folds, use_params, params, save)\n",
        "    self.trained[model_idx] = True\n",
        "\n",
        "\n",
        "  #모델 전체 입력값 예측\n",
        "  def predict_ml_model(self, X_test):\n",
        "    for model in self.ml_models:\n",
        "      model.predict(X_test)\n",
        "  \n",
        "  #모델 이름 변경\n",
        "  def set_ml_name(self, model_idx, name):\n",
        "    self.ml_models[model_idx].set_name(name)\n",
        "\n",
        "\n",
        "#Y인덱스만 남기고 훈련\n",
        "#훈련되지 않은 모델은 훈련 진행\n",
        "\n",
        "def meta_fit(self,n_folds) :\n",
        "  Y_preds = []\n",
        "\n",
        "  #학습된 Y_pred를 Y_preds에 추가\n",
        "  for idx, model in enumerate(self.ml_models):\n",
        "    if (not self.trained[idx]):\n",
        "      model.fit(n_folds)\n",
        "      self.trained[idx] = True\n",
        "    Y_preds.append(model.get_train_preds())\n",
        "\n",
        "  Y_preds = np.hstack(Y_preds)\n",
        "  \n",
        "\n",
        "\n",
        "  for y_idx in range(14) :\n",
        "    print(y_idx, end = '')\n",
        "    X_meta_train = Y_preds[:, [i for i in range(Y_preds.shape[1]) if i % 14 == y_idx]]\n",
        "    super().y_fit(n_folds, y_idx, X_meta_train)\n",
        "\n",
        "  print('Meta', self.name, 'Trained!')\n",
        "\n",
        "\n",
        "\n",
        "#메타 예측\n",
        "  def meta_predict(self, X_test):\n",
        "    Y_preds = []\n",
        "    X_test = super().to_np(X_test)\n",
        "\n",
        "    for idx,model in enumerate(self.ml_models) :\n",
        "      print(model.name, 'Predict Started!')\n",
        "      model.predict(X_test)\n",
        "      Y_preds.append(model.get_test_preds())\n",
        "\n",
        "    Y_preds = np.hstack(Y_preds)  \n",
        "\n",
        "    print('Meta Train Started!')\n",
        "\n",
        "    for y_idx in range(14) :\n",
        "      X_meta_test = Y_preds[:,[i for i in range(Y_preds.shape[1]) if i % 14 == y_idx]]\n",
        "      super().y_predict(X_meta_test,y_idx)\n",
        "    return np.hstack(self.test_preds)\n",
        "\n",
        "\n",
        "  #점수 확인\n",
        "  def scores(self) :\n",
        "    scores = {}\n",
        "    for model in self.ml_models :\n",
        "      scores[model.name] = model.score()\n",
        "    return scores"
      ],
      "metadata": {
        "id": "ffR8YqX-EwG9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Stacking Ensemble 수행"
      ],
      "metadata": {
        "id": "5rpkHfPdKPLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 메타 모델 생성"
      ],
      "metadata": {
        "id": "0KKId3_JKTZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model = meta_ml_model(Ridge, X_train,Y_train)\n",
        "meta_model.set_name('Meta_Ridge')"
      ],
      "metadata": {
        "id": "iQhGHC_OKTGV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 메타 모델 내에 기반 모델 추가"
      ],
      "metadata": {
        "id": "MH_xWkUbKgeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model.add_new_ml_models(\n",
        "    LinearRegression,Ridge,Lasso,ElasticNet, LassoLars,\n",
        "    OrthogonalMatchingPursuit, BayesianRidge, ARDRegression, GradientBoostingRegressor,\n",
        "    HistGradientBoostingRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor \n",
        ")"
      ],
      "metadata": {
        "id": "J-5C_AzeKR9E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model.add_new_ml_models(\n",
        "  HistGradientBoostingRegressor, XGBRegressor, LGBMRegressor, CatBoostRegressor\n",
        "    )\n",
        "\n",
        "meta_model.set_ml_name(13, 'HistGradientBoostingRegressor_tune')\n",
        "meta_model.set_ml_name(14, 'XGBRegressor_tune')\n",
        "meta_model.set_ml_name(15, 'LGBMRegressor_tune')\n",
        "meta_model.set_ml_name(16, 'CatBoostRegressor_tune')"
      ],
      "metadata": {
        "id": "WkJJAtUfK7d7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 학습된 모델을 불러오거나 직접 학습"
      ],
      "metadata": {
        "id": "o-7ZgIdgNeha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model.load_all_models()\n",
        "meta_model.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUtIxNDWNguJ",
        "outputId": "f64f26f2-89b6-4ef6-b13f-3a765060e53a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression didnt loaded\n",
            "Ridge didnt loaded\n",
            "Lasso didnt loaded\n",
            "ElasticNet didnt loaded\n",
            "LassoLars didnt loaded\n",
            "OrthogonalMatchingPursuit didnt loaded\n",
            "BayesianRidge didnt loaded\n",
            "ARDRegression didnt loaded\n",
            "GradientBoostingRegressor didnt loaded\n",
            "HistGradientBoostingRegressor didnt loaded\n",
            "XGBRegressor didnt loaded\n",
            "LGBMRegressor didnt loaded\n",
            "CatBoostRegressor didnt loaded\n",
            "HistGradientBoostingRegressor_tune didnt loaded\n",
            "XGBRegressor_tune didnt loaded\n",
            "LGBMRegressor_tune didnt loaded\n",
            "CatBoostRegressor_tune didnt loaded\n",
            "Meta_Ridge didnt loaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recursive https://github.com/Microsoft/LightGBM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVbNoPE4vSXu",
        "outputId": "d098117d-8f95-41d8-e02d-02988359677b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LightGBM' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LightGBM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAKiaAZCvVSx",
        "outputId": "dd02b1ea-67da-435b-cdc7-fadbdf3a3452"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LightGBM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 모델을 불러오지 않고 직접 학습하기\n",
        "\n",
        "# GradientBoostingRegressor에 대해 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 8)\n",
        "\n",
        "# HistGradientBoostingRegressor에 대해 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 9)\n",
        "\n",
        "# XGBRegressor에 대해 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 10, True, {'objective': 'reg:squarederror', 'random_state': SEED})\n",
        "\n",
        "# LGBMRegressor에 대해 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 11, True, {'random_state': SEED, 'verbose': -1, 'device': 'gpu'})\n",
        "\n",
        "# CatBoostRegressor에 대해 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 12, True, {'loss_function': 'RMSE', 'logging_level': 'Silent', 'random_state': SEED})\n",
        "\n",
        "# HistGradientBoostingRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 13, True)\n",
        "\n",
        "# XGBRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 14, True)\n",
        "\n",
        "# LGBMRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(5, 15, True)\n",
        "\n",
        "# CatBoostRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\n",
        "meta_model.slim_fit_one_ml_model(10, 16, True)\n",
        "\n",
        "# 학습 안된 기반 모델 학습\n",
        "meta_model.fit_ml_model(10)\n",
        "\n",
        "# 기반 모델의 예측 결과를 최종 데이터 세트로 하여 메타 모델 학습\n",
        "meta_model.meta_fit(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "0Poz_hgTTDD1",
        "outputId": "3cc5f1fd-8843-41d1-dd1a-0f140fa8d25c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GradientBoostingRegressor Started\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 GradientBoostingRegressor Slim!\n",
            "HistGradientBoostingRegressor Started\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 HistGradientBoostingRegressor Slim!\n",
            "XGBRegressor Started\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 XGBRegressor Slim!\n",
            "LGBMRegressor Started\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 LGBMRegressor Slim!\n",
            "CatBoostRegressor Started\n",
            "0 1 2 3 4 5 6 7 8 9 10 11 12 13 CatBoostRegressor Slim!\n",
            "HistGradientBoostingRegressor_tune Started\n",
            "HistGradientBoostingRegressor_tune 0 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 1 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 2 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 3 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 4 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 5 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 6 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 7 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 8 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 9 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 10 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 11 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 12 params didnt loaded\n",
            "HistGradientBoostingRegressor_tune 13 params didnt loaded\n",
            "Params Loaded!\n",
            "0 "
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-111a3e74c782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# HistGradientBoostingRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmeta_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim_fit_one_ml_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# XGBRegressor_tune에 대해 하이퍼파라미터 튜닝된 경량화 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-c38b0a82b886>\u001b[0m in \u001b[0;36mslim_fit_one_ml_model\u001b[0;34m(self, n_folds, model_idx, use_params, params, save)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0;31m#머신러닝 모델 하나 경량 훈련(파라미터 입력 가능)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mslim_fit_one_ml_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-04a6fe30e33b>\u001b[0m in \u001b[0;36mslim_fit\u001b[0;34m(self, n_folds, use_params, params, save)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslim_y_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-04a6fe30e33b>\u001b[0m in \u001b[0;36mslim_y_fit\u001b[0;34m(self, n_folds, y_idx, X_train, param, save)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0mtrain_fold_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ABCMeta object argument after ** must be a mapping, not NoneType"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 성능평가"
      ],
      "metadata": {
        "id": "Par8LaF6TKvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model.scores()"
      ],
      "metadata": {
        "id": "V4ixAMVBTMW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_model.score()"
      ],
      "metadata": {
        "id": "_pyFuLSUTOd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 테스트 데이터 예측"
      ],
      "metadata": {
        "id": "gZEhXCHYTQfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ESAA-2/DATA/자율주행_회귀/test.csv').drop(columns = ['ID'])\n",
        "\n",
        "X_test = test.drop([['X_02', 'X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48'], axis=1)\n",
        "\n",
        "#PCA 변환\n",
        "X_test = pca_5.transform(X_test)\n"
      ],
      "metadata": {
        "id": "v9lXDQExTSUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "Y_pred = meta_model.meta_predict(X_test)"
      ],
      "metadata": {
        "id": "_1Ai0ztNTg4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}